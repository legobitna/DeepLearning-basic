{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 8.2b_Building_Transfer_Learning_CNN_Exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3c2d8bf642934efdb53ca801bfe0bfe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_62709e9407084f85b495514e906d6e11",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7d83b54bb26f4fd395e95e1bce9963be",
              "IPY_MODEL_3eef877d22084971b822fbd0801c133e"
            ]
          }
        },
        "62709e9407084f85b495514e906d6e11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d83b54bb26f4fd395e95e1bce9963be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ec0c37bf71b24c22860f7bee410524a0",
            "_dom_classes": [],
            "description": "Dl Completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1c61be0392404999814771573d8a5116"
          }
        },
        "3eef877d22084971b822fbd0801c133e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b44127d5d8a546bb80801c6801e66855",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/4 [00:06&lt;00:00,  1.59s/ file]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_08074b5d31a3466aaeca899f5fc1bafe"
          }
        },
        "ec0c37bf71b24c22860f7bee410524a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1c61be0392404999814771573d8a5116": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b44127d5d8a546bb80801c6801e66855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "08074b5d31a3466aaeca899f5fc1bafe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/legobitna/DeepLearning-basic/blob/main/8_2b_Building_Transfer_Learning_CNN_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKUIp-U7zTc4"
      },
      "source": [
        "# Intro to Convolutional Neural Networks\n",
        "\n",
        "CNNs are very similiar to Neural Networks: they are made up of neurons that have learnable weights and biases. And they still have a loss function on the last (fully-connected) layer and all the tips/tricks we developed for learning regular Neural Networks still apply.\n",
        "\n",
        "So what changes? **CNN architectures make the explicit assumption that the inputs are images**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMjrc9kS0yAI"
      },
      "source": [
        "## Architecture Overview\n",
        "\n",
        "![3-layer Neural Network](http://cs231n.github.io/assets/nn1/neural_net2.jpeg)\n",
        "\n",
        "**Regular Neural Nets don't scale well to full images**. If the input images have the size 32x32x3 (32 wide, 32 high, 3 color channels), so a single neuron in a first hidden layer would have `32*32*3 = 3072` weights. How about 200x200x3 or larger? Clearly, this is wasteful and the huge number of parameters would quickly lead to overfitting.\n",
        "\n",
        "![](http://cs231n.github.io/assets/cnn/cnn.jpeg)\n",
        "\n",
        "**3D volumns of neurons**. CNN arranges its neurons in three dimensions (width, height, depth). Every layer of a CNN transforms the 3D input volumn to a 3D output volumn of neuron activations.\n",
        "\n",
        "![alt text](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/ca8f34f3d28ea528.gif)\n",
        "\n",
        "**Example Architecture**: Overview. We will go into more details below, but a simple ConvNet classification could have the architecture `[INPUT - CONV - RELU - POOL - FC]`. In more detail:\n",
        "\n",
        "* INPUT `[32x32x3]` will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B.\n",
        "* CONV layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as [32x32x12] if we decided to use 12 filters (or kernel_size=12).\n",
        "* RELU layer will apply an elementwise activation function, such as the max(0,x) thresholding at zero. This leaves the size of the volume unchanged ([32x32x12]).\n",
        "* POOL layer will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as [16x16x12].\n",
        "* FC (i.e. fully-connected) layer will compute the class scores, resulting in volume of size [1x1x10], where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10. As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.\n",
        "\n",
        "![](http://cs231n.github.io/assets/cnn/convnet.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcgWzO-xIVIT"
      },
      "source": [
        "## Type of layers\n",
        "\n",
        "There are a few distinct types of Layers (e.g. CONV/FC/RELU/POOL are by far the most popular). Each Layer accepts an input 3D volume and transforms it to an output 3D volume through a differentiable function. Each Layer may or may not have parameters (e.g. CONV/FC do, RELU/POOL don’t)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4Un5NVQJdJs"
      },
      "source": [
        "### Convolutional Layer\n",
        "\n",
        "The Conv layer is the core building block of a Convolutional Network that does most of the computational heavy lifting.\n",
        "\n",
        "|![alt text](http://cs231n.github.io/assets/cnn/depthcol.jpeg) | ![alt text](http://cs231n.github.io/assets/nn1/neuron_model.jpeg) |\n",
        "|-|-|\n",
        "\n",
        "Left: An example input volume in red (e.g. a 32x32x3 image), and an example volume of neurons in the first Convolutional layer. Each neuron in the convolutional layer is connected only to a local region in the input volume spatially, but to the full depth (i.e. all color channels). Note, there are multiple neurons (5 in this example) along the depth, all looking at the same region in the input. Right: The neurons from the Neural Network chapter remain unchanged: They still compute a dot product of their weights with the input followed by a non-linearity, but their connectivity is now restricted to be local spatially.\n",
        "\n",
        "If the receptive field (or the filter size) is 5x5, then each neuron in the Conv Layer will have weights to a [5x5x3] region in the input volume, for a total of 5*5*3 = 75 weights (and +1 bias parameter). Notice that the extent of the connectivity along the depth axis must be 3, since this is the depth of the input volume.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05DJZpISWaYJ"
      },
      "source": [
        "#### Depth, Stride and Zero-padding\n",
        "\n",
        "First, the **depth** of the output volume is a hyperparameter: it corresponds to the number of filters we would like to use.\n",
        "\n",
        "Second, we must specify the **stride** with which we slide the filter. When the stride is 1 then we move the filters one pixel at a time. When the stride is 2 (or uncommonly 3 or more, though this is rare in practice) then the filters jump 2 pixels at a time as we slide them around. This will produce smaller output volumes spatially.\n",
        "\n",
        "Sometimes it will be convenient to pad the input volume with zeros around the border. The size of this **zero-padding** is a hyperparameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJOrdG8VXaIB"
      },
      "source": [
        "**Real-world example**: The [Krizhevsky et al.](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks) architecture that won the ImageNet challenge in 2012 accepted images of size `[227x227x3]`. On the first Convolutional Layer, it used neurons with receptive field size `F=11`, stride `S=4` and no zero padding `P=0`. Since `(227 - 11)/4 + 1 = 55`, and since the Conv layer had a depth of `K=96`, the Conv layer output volume had size `[55x55x96]`. Each of the `55*55*96` neurons in this volume was connected to a region of size `[11x11x3]` in the input volume. Moreover, all 96 neurons in each depth column are connected to the same `[11x11x3]` region of the input, but of course with different weights. As a fun aside, if you read the actual paper it claims that the input images were `224x224`, which is surely incorrect because `(224 - 11)/4 + 1` is quite clearly not an integer. This has confused many people in the history of ConvNets and little is known about what happened. The best guess is that Alex used zero-padding of 3 extra pixels that he does not mention in the paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMCx5hUQa0M8"
      },
      "source": [
        "#### Conv Layer Summary\n",
        "\n",
        "To summarize, the Conv Layer:\n",
        "\n",
        "* Accepts a volume of size  $W_1×H_1×D_1$\n",
        "* Requires four hyperparameters:\n",
        "    * Number of filters $K$,\n",
        "    * their filter size $F$,\n",
        "    * the stride $S$,\n",
        "    * the amount of zero padding $P$.\n",
        "* Produces a volumn of size $W_2×H_2×D_2$ where:\n",
        "    * $W_2=(W_1−F+2P)/S+1$\n",
        "    * $H_2=(H_1−F+2P)/S+1$ (i.e. width and height are computed equally by symmetry)\n",
        "    * $D_2=K$\n",
        "* It introduces $F⋅F⋅D_1$ weights per filter, for a total of $(F⋅F⋅D_1)⋅K$ weights and $K$ biases.\n",
        "* In the output volume, the $d$-th depth slice (of size $W_2×H_2$) is the result of performing a valid convolution of the $d$-th filter over the input volume with a stride of $S$, and then offset by $d$-th bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtHUw-0tcoeJ"
      },
      "source": [
        "### Pooling layers\n",
        "\n",
        "The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation. The most common form is a pooling layer with filters of size 2x2 applied with a stride of 2 downsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations. Every MAX operation would in this case be taking a max over 4 numbers (little 2x2 region in some depth slice). The depth dimension remains unchanged. More generally, the pooling layer:\n",
        "\n",
        "* Accepts a volume of size  $W_1×H_1×D_1$\n",
        "* Requires four hyperparameters:\n",
        "    * the filter size $F$,\n",
        "    * the stride $S$,\n",
        "* Produces a volumn of size $W_2×H_2×D_2$ where:\n",
        "    * $W_2=(W_1−F)/S+1$\n",
        "    * $H_2=(H_1−F)/S+1$ \n",
        "    * $D_2=D_1$\n",
        "* Introduces zero parameters since it computes a fixed function of the input\n",
        "* For Pooling layers, it is not common to pad the input using zero-padding\n",
        "\n",
        "It is worth noting that there are only two commonly seen variations of the max pooling layer found in practice: A pooling layer with $F=3,S=2$ (also called overlapping pooling), and more commonly $F=2,S=2$. Pooling sizes with larger receptive fields are too destructive.\n",
        "\n",
        "In addition to max pooling, the pooling units can also perform other functions, such as average pooling or even L2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to the max pooling operation, which has been shown to work better in practice.\n",
        "\n",
        "| ![alt text](http://cs231n.github.io/assets/cnn/pool.jpeg) | ![alt text](http://cs231n.github.io/assets/cnn/maxpool.jpeg) |\n",
        "|-|-|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNDPOLkvetPC"
      },
      "source": [
        "### The final layer\n",
        "\n",
        "After the last convolutional layer, the data is in the form of a \"cube\". There are two ways of feeding it through the final dense layer.\n",
        "\n",
        "The first one is to flatten the cube of data into a vector and then feed it to the softmax layer. Sometimes, you can even add a dense layer before the softmax layer. This tends to be expensive in terms of the number of weights. A dense layer at the end of a convolutional network can contain more than half the weights of the whole neural network.\n",
        "\n",
        "![alt text](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/a44aa392c7b0e32a.png)\n",
        "\n",
        "Instead of using an expensive dense layer, we can also split the incoming data \"cube\" into as many parts as we have classes, average their values and feed these through a softmax activation function. This way of building the classification head costs 0 weights. In Keras, there is a layer for this: `tf.keras.layers.GlobalAveragePooling2D()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JntrKXFMfUCh"
      },
      "source": [
        "### Layer Pattern\n",
        "\n",
        "![alt text](https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/img/e1a214a170957da1.png)\n",
        "\n",
        "The most common form of a ConvNet architecture stacks a few CONV-RELU layers, follows them with POOL layers, and repeats this pattern until the image has been merged spatially to a small size. At some point, it is common to transition to fully-connected layers. The last fully-connected layer holds the output, such as the class scores. In other words, the most common ConvNet architecture follows the pattern:\n",
        "\n",
        "```\n",
        "INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABassfzTgM1o"
      },
      "source": [
        "> The padding parameter in convolutional layers can have two values:\n",
        ">\n",
        "> * \"same\": pad with zeros so as to produce outputs of the same width/height as the input\n",
        "> * \"valid\": no padding, only use real pixels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xutZgH1Kgcze"
      },
      "source": [
        "## CNN Architectures\n",
        "\n",
        "There are several architectures in the field of Convolutional Networks that have a name. The most common are:\n",
        "\n",
        "* **LeNet.** The first successful applications of Convolutional Networks were developed by Yann LeCun in 1990’s. Of these, the best known is the [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) architecture that was used to read zip codes, digits, etc.\n",
        "\n",
        "* **AlexNet.** The first work that popularized Convolutional Networks in Computer Vision was the [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks), developed by Alex Krizhevsky, Ilya Sutskever and Geoff Hinton. The AlexNet was submitted to the [ImageNet ILSVRC challenge](http://www.image-net.org/challenges/LSVRC/2014/) in 2012 and significantly outperformed the second runner-up (top 5 error of 16% compared to runner-up with 26% error). The Network had a very similar architecture to LeNet, but was deeper, bigger, and featured Convolutional Layers stacked on top of each other (previously it was common to only have a single CONV layer always immediately followed by a POOL layer).\n",
        "\n",
        "* **ZF Net**. The ILSVRC 2013 winner was a Convolutional Network from Matthew Zeiler and Rob Fergus. It became known as the [ZFNet](http://arxiv.org/abs/1311.2901) (short for Zeiler & Fergus Net). It was an improvement on AlexNet by tweaking the architecture hyperparameters, in particular by expanding the size of the middle convolutional layers and making the stride and filter size on the first layer smaller.\n",
        "\n",
        "* **GoogLeNet**. The ILSVRC 2014 winner was a Convolutional Network from [Szegedy et al.](http://arxiv.org/abs/1409.4842) from Google. Its main contribution was the development of an Inception Module that dramatically reduced the number of parameters in the network (4M, compared to AlexNet with 60M). Additionally, this paper uses Average Pooling instead of Fully Connected layers at the top of the ConvNet, eliminating a large amount of parameters that do not seem to matter much. There are also several followup versions to the GoogLeNet, most recently [Inception-v4](http://arxiv.org/abs/1602.07261).\n",
        "\n",
        "* **VGGNet**. The runner-up in ILSVRC 2014 was the network from Karen Simonyan and Andrew Zisserman that became known as the [VGGNet](http://www.robots.ox.ac.uk/~vgg/research/very_deep/). Its main contribution was in showing that the depth of the network is a critical component for good performance. Their final best network contains 16 CONV/FC layers and, appealingly, features an extremely homogeneous architecture that only performs 3x3 convolutions and 2x2 pooling from the beginning to the end. A downside of the VGGNet is that it is more expensive to evaluate and uses a lot more memory and parameters (140M). Most of these parameters are in the first fully connected layer, and it was since found that these FC layers can be removed with no performance downgrade, significantly reducing the number of necessary parameters.\n",
        "\n",
        "* **ResNet**. [Residual Network](http://arxiv.org/abs/1512.03385) developed by Kaiming He et al. was the winner of ILSVRC 2015. It features special skip connections and a heavy use of batch normalization. The architecture is also missing fully connected layers at the end of the network. ResNets are currently by far state of the art Convolutional Neural Network models and are the default choice for using ConvNets in practice (as of May 10, 2016)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zutpbWYRjJY9"
      },
      "source": [
        "### Practice\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKAtU1V9rLrG",
        "outputId": "34cc5baa-75b8-447e-a098-93090727c03a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bN4ECaTpoQ3d"
      },
      "source": [
        "**TensorFlow Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr0dym2_JpiL",
        "outputId": "991b2271-7121-4891-b01c-220fc30a7917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202,
          "referenced_widgets": [
            "3c2d8bf642934efdb53ca801bfe0bfe0",
            "62709e9407084f85b495514e906d6e11",
            "7d83b54bb26f4fd395e95e1bce9963be",
            "3eef877d22084971b822fbd0801c133e",
            "ec0c37bf71b24c22860f7bee410524a0",
            "1c61be0392404999814771573d8a5116",
            "b44127d5d8a546bb80801c6801e66855",
            "08074b5d31a3466aaeca899f5fc1bafe"
          ]
        }
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "mnist = tfds.image.MNIST()\n",
        "\n",
        "# Describe the dataset with DatasetInfo\n",
        "assert mnist.info.features['image'].shape == (28, 28, 1)\n",
        "assert mnist.info.features['label'].num_classes == 10\n",
        "assert mnist.info.splits['train'].num_examples == 60000\n",
        "assert mnist.info.splits['test'].num_examples == 10000\n",
        "\n",
        "# Download the data, prepare it, and write it to disk\n",
        "mnist.download_and_prepare()\n",
        "\n",
        "# Load data from disk as tf.data.Datasets\n",
        "datasets = mnist.as_dataset()\n",
        "train_dataset, test_dataset = datasets['train'], datasets['test']\n",
        "assert isinstance(train_dataset, tf.data.Dataset)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
            "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c2d8bf642934efdb53ca801bfe0bfe0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Dl Completed...', max=4.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdl4WwdJrVfP",
        "outputId": "39a295e5-a627-4d18-8fed-fe6129ab0713",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_dataset.element_spec"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None),\n",
              " 'label': TensorSpec(shape=(), dtype=tf.int64, name=None)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izAuVTOGraKj",
        "outputId": "d0be5f5e-e1cc-404b-99d2-d546b66ce13b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "info = mnist.info\n",
        "print(info)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='mnist',\n",
            "    version=3.0.1,\n",
            "    description='The MNIST database of handwritten digits.',\n",
            "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
            "    }),\n",
            "    total_num_examples=70000,\n",
            "    splits={\n",
            "        'test': 10000,\n",
            "        'train': 60000,\n",
            "    },\n",
            "    supervised_keys=('image', 'label'),\n",
            "    citation=\"\"\"@article{lecun2010mnist,\n",
            "      title={MNIST handwritten digit database},\n",
            "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
            "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
            "      volume={2},\n",
            "      year={2010}\n",
            "    }\"\"\",\n",
            "    redistribution_info=,\n",
            ")\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mBfCp6zrdlH",
        "outputId": "adafc76b-ae24-4eee-ee48-0261ce6f3ee1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tfds.show_examples(info, test_dataset)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: For consistency with `tfds.load`, the `tfds.show_examples` signature has been modified from (info, ds) to (ds, info).\n",
            "The old signature is deprecated and will be removed. Please change your call to `tfds.show_examples(ds, info)`\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAIFCAYAAACtXuUzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de7zVY97/8felrZJS5NBxhJRDdGCqnxSim9uhhtE43IWKm07kNE4Tt+PQGDHlQehOplFICRWhlBgk0l0koShSURPJTtP1+2Ovsr/fz7Vba6+99lpr7/16Ph7zeHS99/X97gtXaz5996fr67z3AgAAVdsuuV4AAADIPQoCAABAQQAAACgIAACAKAgAAIAoCAAAgKSC0kx2zvF3FGF4712u11AW7GuUYJ33fp9cL6Is2NsIKekzmycEABC2ItcLALKJggAAAFAQAAAACgIAACAKAgAAIAoCAAAgCgIAACAKAgAAIAoCAAAgCgIAACAKAgAAIAoCAAAgCgIAACAKAgAAIAoCAAAgCgIAACAKAgAAIKkg1wuA1LBhQ5PttddeJtu6dWtk/Mknn5TbmpCf2rVrZ7J+/fqZrH///pHxlClTzJwZM2akvY6PPvooMp49e3ba9wKQH3hCAAAAKAgAAAAFAQAAEAUBAACQ5Lz3qU92LvXJCGrevLnJZs2aZbJQo+Evv/wSGT/00ENmzlVXXVWG1aXHe++y/k0zKF/3dZs2bUw2bdo0k+23337ZWE7E+vXrI+M5c+aYOffdd5/JVq5cabLly5dnbF0ZNt97f3SuF1EW+bq3kVslfWbzhAAAAFAQAAAACgIAACAKAgAAIJoKjS5dupjsmWeeMVno39uYMWOS3q9Vq1ZmTu3atVO6f1y8yVCS3nzzTZOddNJJSe9VFjQVll2ogXDSpEkm23///bOxnKSci/4nT/VzJH7CoSQ9+eSTkfG9995r5oT2ehbQVJiG+GfczJkzzZxRo0aZbOjQoeW2pkzr1auXyXr27Gmyvn37Rsbfffddua2pNGgqBAAAJaIgAAAAFAQAAIAeAtWrVy8ynj9/vpnTrFkzk5Xm31txX3/9tclSPUzolltuiYwPPfRQMyf0BrtTTz01xdWlhx6Csvvwww9NFuo3yRfp9hCkYsSIESYbMmRIxu5fCvQQpCF+INWVV15p5ixcuNBkPXr0MFm+Hlq1ePFikx122GEmmzhxYmQc6jPIBXoIAABAiSgIAAAABQEAAKAgAAAAkgpyvYBsat++vcnuuOOOyLgsB7+EDib6/PPPk85ZvXp1Sve//fbbk8757LPPUroXqpYlS5aYLNTEVVhYaLLzzjvPZJ07d46M4825knTMMceUZok7DBgwwGTxJkZJuvrqq022devWtL4n0hP6796kSZO0rqtRo0ZG1pRpoabyWrVqpXTtiSeemOHVlC+eEAAAAAoCAABAQQAAAERBAAAAVMWaCkMn9qXS9BF6g2Co0WrVqlXpLSxFe+21V2QcarT6/vvvy3UNyIzTTjstMs70WwzXrl0bGXfv3t3MSbUBddiwYUmz+N6UpBNOOMFkjzzyiMniDWbVqlUzcwYNGmSy4cOHmyxfT7arrI488kiTpXIa3xNPPGGyTz75JCNryrRLLrnEZKFGw8qAJwQAAICCAAAAUBAAAABREAAAAFWxpsLQKyufeeaZyHjRokVmTvw0w2y4+OKLTbbHHntExqFXzj711FPltiZkzm9+85vIuE6dOhm9//jx4yPj8j7BMtTM+uyzz5rs4IMPNtmdd96Z1vd84YUXTHbGGWeYjEbD8hNqEq3oWrduHRmHTs5M1YoVK8q6nKziCQEAAKAgAAAAFAQAAEAUBAAAQJILNaaVONm51CejTGbOnGmyLl26RMavvfaamRM/AU8q/1fCeu/tkYkVSC729ebNmyPj6tWrZ/T+S5cujYwPPfTQjN4/XaFX3MZPUZwwYULa9583b57JOnbsmO7t5nvvj057MXmgvPf2hg0bTFa3bt2k14UatYcOHZqRNZVV+/btI+N33nkn7XvFXxM+d+7ctO+VSSV9ZvOEAAAAUBAAAAAKAgAAoCp2MFG+6tChg8kOO+ywpNc9+uijJivvfgFkRvxn6aXp5UlF/O2JvXr1MnPGjRuX0e+ZisLCQpPFe2HeeustM+eYY45J6f41a9ZMb2FI6tZbbzVZ7dq1k14XOhTr4YcfzsiakFk8IQAAABQEAACAggAAAIiCAAAAiKbCrGvVqpXJpk6darJ69eqZbM6cOZHxjBkzMrcwVCrxpsXGjRvnaCXJxd+UGDrsBrkXb1SVpGrVqiW9rlatWiZr0qSJyVatWpXewpAxPCEAAAAUBAAAgIIAAACIggAAAKiSvO3w8MMPN9nvfvc7k8XfqiZJRx+d/GVmu+xi66Zt27aZLPSmtXh23nnnmTn169c3WaixqkePHpFxvMkwV3jbYenF/9sde+yx5fr9brzxRpPdfffd5fo90xVqXvviiy9M5pzddqHPs4EDB0bGDz30UKpL4W2HxYQaokOfQXvuuWfSe4X+ey5btiy9hWVY/G2N8bcflsasWbMi41NOOcXM2bJlS9r3TxdvOwQAACWiIAAAABQEAACAggAAAKgCnFR49tlnR8YDBgwwc4477jiTpdosmcq8UANh6LpQg2IqTYuh+4f+OfOliRBlN378+Mi4U6dOad8r1Mz6zTffRMajR49O+/7ZduCBB5os9Pstk7/HkdyiRYtMFnot9XPPPRcZt2zZ0sw54IADUsoquhNOOCEyDr32uW/fvtlaTlI8IQAAABQEAACAggAAAIiCAAAAKM+aCs8880yTPfHEE5Fx9erVzZy1a9eaLNRINGbMGJP9/PPPkfGECRPMnPXr15vstttuM9kll1xisnR9/fXXGbsXKreePXua7KuvvsrBSjLjqquuSvva0D/3q6++WpblYCeWLFlisnPPPTcyPumkk8ycv/zlL+W2pnzy448/RsahpsJ8whMCAABAQQAAACgIAACActhDED9wSLL9ApLtGQj1AWTyZ/chN998s8lC/Q6Z9F//9V8m++c//xkZ5+ItWUCmNW/ePDI+6KCD0r5X6C2h+fIWvapiwYIFkfHChQvNnJEjR5rsr3/9q8mWLl1qslGjRkXGnTt3NnOuueaapOssyfHHHx8Zh/rWQh544AGTXXfddZFxYWFh2uvKBp4QAAAACgIAAEBBAAAAREEAAAAkudK8Ccw5l7HXhs2cOdNkXbp0MVm8iXDQoEFmTlkaNRo3bhwZ33TTTWbOpZdearLQv7fQW+fuuuuuyLhPnz5mTo8ePVK6/5VXXhkZjxgxwszJBe+9y/UayiKT+zpVtWvXjozfffddMyf0lriQcePGmezCCy9Mb2HlLN5AKEkvvvhiZHzwwQenff8nn3zSZL179073dvO998lfV5rHcrG3K7r4m0IbNGhg5qxbt85koc/xt956K3MLy6CSPrN5QgAAACgIAAAABQEAABAFAQAAUJaaCo899liTzZ4922SffPKJyQ477LB0vqWaNWtmsvgJVJJ04403RsahU9JCJwLee++9JpsyZYrJ3nvvvZ2sssh3331nsnr16plszpw5kXGoiWXjxo1Jv1+m0VRYdh9++KHJWrVqldK1a9asMdkrr7wSGQ8ePNjM+de//pXi6pKrWbOmyfbff3+TTZ482WSpNk/GrVy50mS///3vTZbK78ES0FRYBaXSVPjZZ5+ZLNQwm69oKgQAACWiIAAAABQEAACAggAAAChLrz8Onf4XamacMGFC0nuFGjdOPPFEk8VPCJSkunXrJr3/yy+/bLLQ64/L0KhknHrqqSZ77rnnTBZ/zeeDDz5o5pThVDbkUKghNdWmwn333ddk8ddnN2nSxMx5++23Tfb888+brHv37iZzLtqTFLr/+eefbxebQUcccYTJctFUC1QWPCEAAAAUBAAAgIIAAAAoSwcT/fvf/zZZ6PuGDiuKH3gS+rlq/M1xkvTzzz+b7NtvvzVZ/Oecod6ArVu3mqy8TZo0yWRnnHFGZPzll1+aOaG3QU6fPj1zCwvgYKKyKyiw7Tzxt1tK0t13312u6wj9vgkdOrTLLtE/S2zbti1jawjt/X79+pnshx9+MFlpPs9SwMFElVyoP2zo0KGRcej3JgcTAQCASouCAAAAUBAAAAAKAgAAoCwdTDRmzBiTXXTRRSY77rjjTPbRRx9Fxo8//riZ88Ybb5gs9Ca00EEs+eqss84y2dixYyPj+OEzktSmTRuTlXdTIcou1Lg6fPhwk4UaaK+77jqT7brrrmmtI9RAGJJu897atWtNFn8z4+WXX27mcOAQykPDhg1NFmoijAsdHFcZ8IQAAABQEAAAAAoCAAAgCgIAAKAsnVRYo0YNkx100EEpXRtvDqzKzUX77LPPTsdS+AStwsLCcluTxEmFudarVy+TNW3aNDK+4447Mvo94ycVLl261MwJNUV+8MEHJnvnnXcyt7DM4qTCSu6hhx4y2WWXXZb0utCJuYsXL87ImrKBkwoBAECJKAgAAAAFAQAAoCAAAADK0kmFoaa2+AmESC5+ylvo1DdUPePGjUs6589//nMWVgKgIuMJAQAAoCAAAAAUBAAAQBQEAABAWWoqBAAg34ROKmzXrl1kfOedd5o5X375ZbmtKZd4QgAAACgIAAAABQEAAFCW3naIyo23HaKS4m2HqJR42yEAACgRBQEAAKAgAAAAFAQAAEAUBAAAQBQEAABAFAQAAEAUBAAAQBQEAABApX/b4TpJK8pjIaiw9s/1AjKAfY0Q9jYqoxL3damOLgYAAJUTPzIAAAAUBAAAgIIAAACIgmAH51xT59ws59xHzrnFzrkrdjJ3iHPugsSv/+KcW+KcW+icm+ycq5fIj3DOPZ6l5QMlcs6d4pz7xDm3zDl3/U7m3e+c65L49QHOuXcS1zzlnKueyAc55/pma+1AMs65as65D5xzL+5kzo69XSz7m3Pux2LjKr+3KQh+tVXS1d77wyR1lDTQOXdYfJJzrkBSX0lPJqJXJLXy3h8paamkGyTJe/9/kpo4536TjcUDIc65apIelPSfkg6TdF4J+7q+pI7e+zmJ6B5Jw733zSWtl9Qvkf+vpMHlvnAgdVdI+rikLwb2tpxzR0vaMza1yu9tCoIE7/033vv3E7/+QUUbrHFgaldJ73vvtybmztj+a0lvS2pSbO4Lks4tv1UDSbWXtMx7/7n3foukCZJ6BOb9XtJLkuSccyra5xMTXxsr6XeS5L3/SdJy51z78l44kIxzromk0yQ9tpNpO/Z24ppqkv4i6Y/FJ7G3KQiCnHPNJLWV9E7gy50kzS/h0r6SphcbvyepcybXBpRSY0lfFRuvVLjQLb6v60vaUKzQjV/Dvka+uF9F/8e+bSdz4p/ZgyQ9773/JjC3Su9tCoIY51xtSc9KGuK93xiY0lDS2sB1N6noxw7/KBavkdSoPNYJZFhwX5eAfY2cc86dLmmN976kP6Btt2NvO+caSeopaUQJc6v03i7tSYWVmnNuVxUVA//w3k8qYdpmSTVj110k6XRJJ/roSU81E/OBXFklqWmxcZNEFld8X38nqZ5zriDxlCB+Dfsa+aCTpO7OuVNVtCf3cM6N8973is0rvrfbSmouaVnRT8ZUyzm3LNErI1Xxvc0TgoTEz01HS/rYe3/fTqZ+rKINtf26U1T0yKp74mdQxbWQtCjTawVKYZ6kgxN/a6C6inpang/M27GvE0XtLElnJ752oaQpxeayr5Fz3vsbvPdNvPfNVLSvZwaKASm6t6d67xt475slrvupWDEgVfG9TUHwq06Sekvq6pxbkPjfqYF50yUV/+srIyXVkfRK4pqHi33tBElTy23FQBKJP+EPkvSyij4Yn/beLw5MnSrp+GLj6yRd5ZxbpqKegtHFvtZJRX+7BqgI4nt7Z6r03uZdBmlwzk2W9Efv/ac7mVND0mxJxxZrzgLylnNurqTTvfcbdjKnraSrvPe9s7cyoGzY26mhIEiDc66lpP2K/73WwJyDJTX23r+etYUBZeCc6yBps/d+4U7mdJP0qfd+edYWBpQRezs1FAQAAIAeAgAAQEEAAABEQQAAAERBAAAAREEAAABEQQAAAERBAAAAREEAAABEQQAAAERBAAAAREEAAABEQQAAAERBAAAAREEAAABEQQAAAERBAAAAREEAAABEQQAAAERBAAAAREEAAABEQQAAAERBAAAAREEAAABEQQAAAERBAAAAREEAAABEQQAAAERBAAAAJBWUZrJzzpfXQlBxee9drtdQFuxrlGCd936fXC+iLNjbCCnpM5snBAAQtiLXCwCyiYIAAABQEAAAAAoCAAAgCgIAACAKAgAAIAoCAAAgCgIAACAKAgAAIAoCAAAgCgIAACAKAgAAIAoCAACgUr7tEACAqqRbt24mGzhwoMm6d+9usmHDhkXG119/feYWVg54QgAAACgIAAAABQEAABAFAQAAEE2FAIAqqmHDhiY7+eSTI+P77rvPzKlbt67JvPcmGzJkSGT86aefmjmjR49Ous5s4QkBAACgIAAAABQEAABAFAQAAEA0FQIAKpnatWubrFevXibr27evyY466qiMraNatWqRcZ06dTJ27/LAEwIAAEBBAAAAKAgAAIAoCAAAgCQXOl2pxMnOpT4ZQfvtt5/Jzj//fJO1adMm6b1GjBhhsvfeey+9hZWB995l/ZtmEPsaJZjvvT8614soi6q6t+fMmWOyTp06mcw5+9EV///EwsJCM2f48OEmC70Sef369ZHxAQccYBebAyV9ZvOEAAAAUBAAAAAKAgAAIA4myqj4IRSS9Mc//jEyvuqqq8yc0M+x9tprr6Tfr1GjRibr1q1b0uuQe4ccckhkfP/995s5jRs3NlmoRyR07YcffliG1QH5K/57Z8qUKWZO06ZN077/999/HxlfcsklZs5zzz1nstCbE8ePH5/2OnKBJwQAAICCAAAAUBAAAABREAAAAHEwUdpat25tsv/5n/8xWY8ePSLjsWPHmjm33nqryb766iuTPfHEE5Fx165dzZxQY0tIgwYNTLZ69eqUro3jYKLSO+644yLj119/Pe17bd261WRLly6NjOfOnZv2/adNm2ayzZs3R8ZnnXWWmZNuQ9Xy5ctNtmLFirTuVUYcTJRjBQW27/2BBx6IjC+77LK07x/6nL3yyisj48mTJ6d9/3zFwUQAAKBEFAQAAICCAAAAUBAAAADRVJiSjh07muzxxx832UEHHWSyeMPLmDFjzJxt27altI74yXXTp083c/r06WOyW265xWShk+yGDh2a0jriaCosvRo1akTGoX1x3nnnZWs5eeWHH34w2bvvvmuyk046qbyXQlNhFsVPIJSkwYMHm6wsTYRxodNlqwKaCgEAQIkoCAAAAAUBAACgIAAAAOL1xym5+uqrTdayZUuTxU8llKTnn38+Y+vYtGlTZBx6/fG8efNMdvPNN5vsvvvuy9i6UHqFhYWRcd++fc2c2267zWQnn3yyyTZu3GiyCy64IDIuy+tgUxE6ITPULFu7du2k96pTp47JPvjgg/QWhgojlabsVL3yyismGzFiRFr3qkp4QgAAACgIAAAABQEAABAFAQAAEE2FRrNmzUwWerXrqFGjTPbCCy9kbB2/+c1vTBZvitlzzz1TWte9995rsp9//rkMq0Omhf57LFmyJKUsJHTyYXlq0aKFyQ499FCTTZo0yWS77BL9c8m///1vM2fRokVlWB0qglDzdio2bNhgsuuvv95kCxYsSOv+VQlPCAAAAAUBAACgIAAAAKKHwGjQoIHJnLMvhpo9e7bJQm+OLCiI/ivu37+/mdO1a1eTnXLKKSZbtmxZZHz22WebOZMnTzYZUN4+/fRTk919990mi/cLSPb3zbXXXmvmjB07tgyrQ0Vw4IEHpnVd/BAuiX6BdPGEAAAAUBAAAAAKAgAAIAoCAAAgmgqNNm3apDRv3bp1Jgu9mWvgwIGR8eGHH27mrF+/3mT33HOPyeIHE3333XdJ1wlkw/HHH2+yM888M6Vr42/eHD58eCaWhDx2ww03mCx0GFsq3njjjbTX0apVK5N17tw56XWht45279496XVTpkwx2TnnnGOyLVu2JL1XeeAJAQAAoCAAAAAUBAAAQBQEAABANBUa9evXT2neiy++aLL4qYSS9MEHH0TGffr0MXMmTJhgssLCwpTWAeTCxRdfHBk/+uijKV33/fffm+zOO+/MyJqQn0Kfi6EGwtBJryH3339/ZLxp0yYzp3Xr1iarU6eOyZ566imThU6rTUUq6w81HtasWdNkNBUCAICcoSAAAAAUBAAAgIIAAACIpkL9x3/8R2R83XXXpXRdqOmjR48eJnvppZfSWxiQJ5o0aWKyK664Iq17XXrppSYLndSJymP33Xc32X//93+nfb+NGzdGxqHXx48bN85ke++9t8lCr7ZPpTkw1PS96667miz0uu98VrFWCwAAygUFAQAAoCAAAAAUBAAAQFWsqbBfv34me+SRRyLjZcuWmTlr1qwx2VFHHWWyUFMJUNE9++yzJgu9Njbu4YcfNtlzzz2XkTWh6rr55puz+v2ef/55k4X29qhRo0zWtGnTcllTeeEJAQAAoCAAAAAUBAAAQJWkh2C//fYz2bBhw0x26qmnmizeV/Dkk0+aOaE3c40ZM8ZkI0eONNm8efMi49WrV5s5QL449thjTRZ6c1zcW2+9ZbL+/ftnZE1AeZk6darJHnzwwcg49JbE0047zWSNGjVK+v2WLFlisq1btya9Llt4QgAAACgIAAAABQEAABAFAQAAUAVsKiwosEteunSpyUJvsQq9Feu9995L+j1DhxWNGDHCZBMmTDBZhw4dIuMpU6Yk/X5ANhx99NEme+2110xWvXp1k40fPz4yHjBgQOYWBpTRL7/8YrL77rvPZH/+859NFn8DbuhzPVWffPJJZNy9e3cz56effkr7/pnGEwIAAEBBAAAAKAgAAIAoCAAAgCpAU2H8DYJz5swxcwoLC00WbwyRpAULFmRsXfXr109p3rp16zL2PYF07bKLrf2feOIJk4UaCN955x2TxZsIN2zYUIbVAZmV6n585plnTNatW7eMrePqq6+OjD/77LOM3bs88IQAAABQEAAAAAoCAAAgCgIAAKA8ayrce++9TXb77bdHxvGT/yTpmGOOMVkmGwhr1Khhst69e5tsy5YtJgudoghkW+h13YceeqjJNm7caLJrrrnGZDQRIlWhU2PL2z777GOy6667zmShZttt27Ylvf/ixYtN9uSTT5rslVdeSXqvfMITAgAAQEEAAAAoCAAAgPKshyB0iE+tWrUi4++//97MqVmzpslCb0UMadOmTWTctGlTMyf0lqzQvNtuu81ka9euTWkdQCYNHDgwMr7gggtSuu5vf/ubyebOnZuRNaFq+vHHH03WpUsXkz3wwAMma9u2bbmsaTvvfdI5oT6wM844w2QrVqzIyJpyiScEAACAggAAAFAQAAAAURAAAABJLpWmih2TnUt9cobEm/5Gjx5t5mSy8SR0KMXs2bNNdvnll5ssdFhFVeC9z/7JIxmUi32dSbvttpvJ4gcHhd5iOGPGDJP17NnTZKHDiqqI+d77o3O9iLKoSHt7//33N9kLL7xgssMPPzxj3/ONN94w2YQJEyLjV1991cxZtmxZxtaQCyV9ZvOEAAAAUBAAAAAKAgAAIAoCAACgCtBUGNegQQOTnXjiiWnfL3661JIlS8yc0AmK+BVNhbl15513muzGG2+MjENNUEceeaTJNm/enLmFVXw0FaJSoqkQAACUiIIAAABQEAAAAAoCAACgCthUiPxDU2H21K9f32TLly83We3atSPjk08+2cwJnVSICJoKUSnRVAgAAEpEQQAAACgIAAAABQEAAJBUkOsFAEjdGWecYbJ4A2FI6DWvAFAcTwgAAAAFAQAAoCAAAACihwCoUFLpFwi55pprTHb77beXdTkAKhGeEAAAAAoCAABAQQAAAERBAAAAxNsOkQG87RCVFG87RKXE2w4BAECJKAgAAAAFAQAAoCAAAAAq/UmF6yStKI+FoMLaP9cLyAD2NULY26iMStzXpfpbBgAAoHLiRwYAAICCAAAAUBAAAABREEQ45650zi12zi1yzo13ztUsYd79zrkuiV+f6Jx73zm3wDk31znXPJEPcs71zeb6gTjnXMvE3tz+v43OuSElzB3inLsg8eunil2z3Dm3IJEf4Zx7PIv/CECJnHP1nHMTnXNLnHMfO+f+XwnzduztxHhw4prFzrlhiazK722aChOcc40lzZV0mPd+s3PuaUnTvPePx+bVlzTVe98xMV4qqYf3/mPn3ABJ7b33Fznnakl603vfNrv/JECYc66apFWSOnjvV8S+ViDpfUntvPdbY1/7q6R/ee9vS4xfldTXe/9ldlYOhDnnxkp6w3v/mHOuuqRa3vsNsTmRve2cO0HSTZJO894XOuf29d6vScyt0nubJwRRBZJ2S2ygWpK+Dsz5vaSXio29pD0Sv667/Rrv/U+Sljvn2pffcoFSOVHSZ/FiIKGrpPcDxYCT9AdJ44vFL0g6t9xWCaTAOVdXUhdJoyXJe78lXgwkxPd2f0l3e+8LE9etKTa3Su9tCoIE7/0qSfdK+lLSNyr6E9GMwNROkuYXG18saZpzbqWk3pLuLva19yR1Lp8VA6V2rqL/x15cfF9v11nSt977T4tl7GvkgwMkrZU0xjn3gXPuMefc7oF58b3dQlJn59w7zrnZzrnfFvtald7bFAQJzrk9JfVQ0SZrJGl351yvwNSGKtqE210p6VTvfRNJYyTdV+xraxL3AnIq8Ti1u6RnSpgS39fbnSdbRLCvkQ8KJLWT9FDiR7ObJF0fmBff2wWS9pLUUdK1kp5OPAmTqvjepiD41UmSvvDer/Xe/yJpkqRjAvM2S6opSc65fSS19t6/k/jaU7FraibmA7n2nyp6bPptCV/fsa+3S/zo7CwV7evi2NfIByslrSz2+TtRRQVCXHxvr5Q0yRd5V9I2SXsnvlal9zYFwa++lNTROVcrUS2eKOnjwLyPJTVP/Hq9pLrOuRaJcbfYNS0kLYmWZPQAABKOSURBVCqn9QKlEfqTfnHF9/V2J0la4r1fGcvZ18g57/1qSV8551omohMlfRSYGt/bz0k6QZISn93VVXTEs1TF9zYFQUKiypyoom7U/1PRv5tHAlOnSjo+cc1WSZdIetY596GKegiuLTa3k6RXym/VQHKJn6t2U9FTr5JMV1GDVnEl9RycoKLfB0CuDZb0D+fcQkltJN0VmBPf2/8r6UDn3CJJEyRd6H/963ZVem/z1w7T4JybK+n0Ejpat89pK+kq733v7K0MSJ9zbrKkP8YaCONzakiaLenY+N9IAPIVezs1FARpcM51kLTZe79wJ3O6SfrUe788awsDyiDx6HU/7/2cncw5WFJj7/3rWVsYUEbs7dRQEAAAAHoIAAAABQEAABAFAQAAEAUBAAAQBQEAABAFAQAAEAUBAAAQBQEAABAFAQAAEAUBAAAQBQEAABAFAQAAEAUBAAAQBQEAABAFAQAAEAUBAAAQBQEAABAFAQAAEAUBAAAQBQEAABAFAQAAEAUBAAAQBQEAABAFAQAAEAUBAAAQBQEAABAFAQAAEAUBAACQVFCayc45X14LQcXlvXe5XkNZsK9RgnXe+31yvYiyYG8jpKTPbJ4QAEDYilwvAMgmCgIAAEBBAAAAKAgAAIAoCAAAgCgIAACAKAgAAIAoCAAAgCgIAACAKAgAAIAoCAAAgCgIAACAKAgAAIAoCAAAgCgIAACAKAgAAIAoCAAAgCgIAACApIJcLwBAxVO7du3I+NFHHzVzzj33XJO9/fbbJjv55JMj440bN5ZxdajqqlevbrIaNWqkdO1JJ50UGd9yyy1mzhFHHJHSveLX3nHHHSldlys8IQAAABQEAACAggAAAIiCAAAAqBI3FRYU2H+0iy++2GQHH3xw0nv9+OOPJnvsscdMtmbNGpMVFhYmvT+Qzw455BCTTZs2LTJu1qyZmeO9N1mHDh1M1rt378j4wQcfLOUKkU+qVasWGbds2dLMufTSS8t1DUceeaTJOnfubDLnnMlC+zadOVJ4v+cznhAAAAAKAgAAQEEAAABEQQAAAFSJmwr/9Kc/pZSlItR4ctNNN5ls1qxZJnv11Vd3Opak+fPnp7UuINMaNmxospdfftlkTZs2jYwfeeQRM+e2224z2bJly0wWagBGxbXvvvtGxgsXLszRSrJr8+bNJps0aVIOVpI+nhAAAAAKAgAAQEEAAAAkuVQPWJAk51zqk7PovPPOM9m4ceNMVpp/1uLSPbwi5JdffjHZ+++/b7KnnnrKZLNnz46MP/zww7TWkGnee/svqALJ131d3nbbbTeTjRkzxmR/+MMfTPbSSy9Fxj179jRzNm3aZLKnn37aZKNGjYqMX3vtNbvY3JjvvT8614soi1zs7XgfyldffZXtJaQsk5/t11xzjcnuv//+tO5V3kr6zOYJAQAAoCAAAAAUBAAAQBQEAABAlaSpcPHixSYLvaEtH5oKy3Kv+FsXx48fb+b0798/rXWVBU2FFVNor4TeNPjFF1+YrHXr1pFx6I2gIaG3Iq5atSoyDjXe5ghNhWmoUaNGZDxy5Egzp0+fPmnf/4MPPoiMQ5/1oYbZkFQ+j0MHDg0dOtRk//jHP0y2du3alNaRbTQVAgCAElEQAAAACgIAAEBBAAAAVAGbCkeMGGGyAQMGmGyXXWyts23btrS+Z+i6r7/+2mQTJkww2bRp0yLj+GmDktSoUSOTnXPOOSa78sorI+MmTZqYOfEGLUk688wzTbZgwQKTbd261WSpoKkw/x19tO2Ne/PNN032ww8/mOzkk082WRV5QydNhRkQavA76qij0r5f/LMrtBcPOuiglO4Vair8+eefI+NBgwaZOaETPSsSmgoBAECJKAgAAAAFAQAAoCAAAACSCnK9gGTq1KkTGXfp0sXMCTVGhhoBQw1TY8eOjYzbtWtn5syYMcNkt99+u11smkINisOHDzfZN998ExmHTsaKv3pUkt5++22TDRw40GTx19Ci8rj88stNtuuuu5rsn//8p8mqSAMhyknopL+5c+emfb/4Xm7atGna9wo1Useb1OP/H1GZ8YQAAABQEAAAAAoCAAAgCgIAAKAKcFLhhRdeGBmPHj06petCJ1ANGTLEZKGTDyuKUFNh6ITDkKlTp5qsR48eaa2DkwrzT/v27SPjt956y8z57LPPTBY60TDUjFtFcFJhjg0ePNhk99xzT2RcvXr1tO9/0UUXmWzcuHFp36+i4KRCAABQIgoCAABAQQAAAPLsYKLQW/9GjhyZ1r1Ch/089thjad0rX61evTrta0MHGKFiCv0M9fHHH4+MQ2///Pvf/26yUL9AzZo1k37PjRs3JlsmsFOhw9KGDRtmstCBWumqCv0CpcETAgAAQEEAAAAoCAAAgCgIAACA8qypsGvXriarVatWWvcKNUeF3rpVkcXfBCmFD2QKmTNnTqaXgxw566yzTHbIIYckva5FixYm++KLL0xWUGA/JqpVqxYZ//zzz2bOhAkTTHbLLbeY7JdfftnpOlH5nHnmmSYbNGiQyTLZQBhyww03pHXd5MmTTbZkyZKyLifneEIAAAAoCAAAAAUBAAAQBQEAAFCeNRW2bdvWZKV5G2Nxjz76aFmXk3dOP/30yLhfv35mTqr/vtL994r8E3pDYSp69eplsi1btpgs9Hsp3kQYfyupJF1//fUme+mll0xGg2vl1rx5c5NNnDgxByux7rrrLpNt27Yt6XV33HGHyZ5++mmTDR06NDJetmxZKVaXfTwhAAAAFAQAAICCAAAAiIIAAAAoz5oKMyl0SlpFd+qpp2bsXvne3IKw0Mmdp512Wlr3WrFihcluvPFGk40fPz7pvZ599lmTvfXWWyYbNWqUyY466qjI+Keffkr6/VCx5UtTc6iBMN219ezZ02Tt27ePjEOnii5evNhkW7duTWsNZcUTAgAAQEEAAAAoCAAAgCgIAACA8qypsF27dmld9/7775vsm2++Ketycurmm282WehkwlQsXbrUZJWx6bIqOOOMM0zWsmXLpNetWrXKZN26dTNZus2m8+fPT2leaK21a9eOjGkqrFzWrFljsptuuslkffr0MVmoiXaPPfaIjGvUqGHmbNq0yWTr1q0zWeh18XvvvXdkXLduXTMnVfvvv39kHPp90rFjR5O99957aX/PsuAJAQAAoCAAAAAUBAAAQHnWQ3DccceZLJVDIir629JatWplsksvvdRkBQXR/1yhn3+F3lYXeqvdv/71r9IsEXmiYcOGaV03ffp0k3E4FbJh48aNJrvnnntSyho0aGCyZs2aRcb16tUzc1avXm2yBQsW7GyZO7Rp0yYy/u1vf2vmDBkyxGSp9PKEhA4DO+ecc0z2yy+/pHX/0uAJAQAAoCAAAAAUBAAAQBQEAABAedZUGGogTKWpMF/enJWKUAPh1KlTTbbffvuZLP7PGWogDDW7hA5uQtUyceLEcr1/qPkrJPRmtx9++CHTy0ElEWoODGWZFG8+DDUjhj6zX3/9dZMdeOCBSb9f9+7dTbbXXnuZ7Ntvv016r7LiCQEAAKAgAAAAFAQAAEAUBAAAQHnWVPjpp5+arHnz5jlYSebE31oYOoEw1ECYisGDB5vsscceS+teqBi+++67tK6bOXNmRtcRPzVz7NixKV3397//3WSbN2/OyJqQH+JvHzzrrLPMnMsuu8xkX375pckeeOABk2X7TYBHHnmkya699lqTpdJAGLJy5UqThRrGs4EnBAAAgIIAAABQEAAAAFEQAAAA5VlTYej0pyuuuCIHK0nu9NNPN9mf/vQnk7Vt2zYyjjdjSamftDhgwIDImAbCqmfGjBlpXbfHHnuY7Pvvv0/p2l133dVk8Uax448/3sxZtWqVyUJNYqhcrrnmmsj41ltvTem6Tp06mSz0Ofv5559HxgsXLjRzpk2bltL3vOGGG0wW/zxu2rSpmRM6STBd559/vsnWr1+fsfuXBk8IAAAABQEAAKAgAAAAyrMegh9//NFkzrmk14V+PpqqWrVqRcb169c3c4YOHWqyfv36pfX9Qv88oUMoOHQIIaGf+8+ePdtkxx13XGQc/7muJN14440mS6VfQJLGjx8fGYd+75522mkmKywsNBkql3333Tdj96pTp47JWrduvdOxJPXu3Tul+4c+jzP59tz4oUMjR440c+bNm5ex71dWPCEAAAAUBAAAgIIAAACIggAAAEhypWmgcM5lrtsiINSMsmjRosg41QMhnn322ZTmNWnSJDLu0KGDmZPJxpNXX33VZPfcc4/JZs2aldb9c8F7n7zzM4+V974ub126dDHZ9OnTI+PQGwXjv7ckaffddzfZUUcdZbJ4E2H37t3NnNdff91kFcx87/3RuV5EWeRib8cPnxo4cGC2l5CyTH62P//88yaLv+029HsuF0r6zOYJAQAAoCAAAAAUBAAAQBQEAABAedZUGNKiRYvIuH///mbOxRdfbLL4CYRS+s0iqTaezJw502TxJsJhw4altYZ8RlNh/mnUqFFk/MQTT5g5Xbt2NdmGDRtM9swzz5hsxIgRkXG+NEtlGE2FaahRo0ZkHHrDa8gf/vAHkx144IFJr7vssstMtueee6b0PefMmWOyN998MzIO/Z54+OGHTRY6hXPr1q0prSPbaCoEAAAloiAAAAAUBAAAgIIAAACoAjQVpqJhw4YmCzVMtWnTJq37b9q0yWShVxGvWbPGZKFXG1c2NBWikqKpEJUSTYUAAKBEFAQAAICCAAAAUBAAAABVkqZC5BZNhaikaCpEpURTIQAAKBEFAQAAoCAAAAAUBAAAQBQEAABAFAQAAEAUBAAAQBQEAABAFAQAAEAUBAAAQBQEAABAFAQAAEAUBAAAQFJBKeevk7SiPBaCCmv/XC8gA9jXCGFvozIqcV+X6vXHAACgcuJHBgAAgIIAAABQEAAAAFEQRDjnTnHOfeKcW+acu34n8+53znVJ/PoA59w7iWuecs5VT+SDnHN9s7V2oCTOuXrOuYnOuSXOuY+dc/+vhHlDnHMXFBsPTlyz2Dk3LJEd4Zx7PEtLB0rknKvpnHvXOfdhYo/eupO5xT+zRyeuWZj4fVE7kVf5z2yaChOcc9UkLZXUTdJKSfMknee9/yg2r76kqd77jonx05Imee8nOOcelvSh9/4h51wtSW9679tm9R8EiHHOjZX0hvf+sUTBWst7vyE2p0DS+5Laee+3OudOkHSTpNO894XOuX2992sSc1+V1Nd7/2WW/1GAHZxzTtLu3vsfnXO7Spor6Qrv/duxefHP7D289xsTv75P0hrv/d18ZvOEoLj2kpZ57z/33m+RNEFSj8C830t6SdqxIbtKmpj42lhJv5Mk7/1PkpY759qX98KBkjjn6krqImm0JHnvt8SLgYSukt733m9NjPtLutt7X5i4bk2xuS9IOrf8Vg0k54v8mBjumvhf6E+4Oz6zE9dtLwacpN22X8NnNgVBcY0lfVVsvDKRxXWSND/x6/qSNhT7EI1f856kzhleJ1AaB0haK2mMc+4D59xjzrndA/OK72tJaiGpc+LHYbOdc78t9jX2NfKCc66ac26BpDWSXvHevxOYFt/bcs6NkbRa0iGSRhT7UpXe2xQEpddQRR+wqVgjqVE5rgVIpkBSO0kPJR6FbpIU6o+J7+sCSXtJ6ijpWklPJ/5EJbGvkSe89//23reR1ERSe+dcq8A085ntve+joj38saRzin2pSu9tCoJfrZLUtNi4SSKL2yypZuLX30mql/j5a+iamon5QK6slLSy2J+cJqqoQIgrvq+3Xzcp8Vj2XUnbJO2d+Br7Gnkl8WOwWZJOCXw5vre3X/NvFf1o+PfF4iq9tykIfjVP0sGJvzVQXUU/I30+MO9jSc2lop9hqWgTnp342oWSphSb20LSonJbMZCE9361pK+ccy0T0YmSPgpM3bGvE56TdIIkOedaSKquomNwJfY18oBzbh/nXL3Er3dTUUP4ksDUHXvbFdnxa0ndY9dU6b1NQZCQ6AMYJOllFW2gp733iwNTp0o6vtj4OklXOeeWqainYHSxr3WS9Eq5LBhI3WBJ/3DOLZTURtJdgTnTVdR8uN3/SjrQObdIRX+KutD/+leSTlDR7wMglxpKmpXY1/NU1EPwYmBe8c9sJ2msc+7/JP1f4h63FZtbpT+z+WuHaXDOzZV0egnd2tvntJV0lfe+d/ZWBqTPOTdZ0h+995/uZE4NSbMlHVusmRbIa3xmp4aCIA3OuQ6SNnvvF+5kTjdJn3rvl2dtYUAZJH6ssJ/3fs5O5hwsqbH3/vWsLQwoIz6zU0NBAAAA6CEAAAAUBAAAQBQEAABAFAQAAEAUBAAAQNL/BwbOhv+SEbocAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 648x648 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x648 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DEL_yAirhZm",
        "outputId": "25a94c50-bc2d-4f41-f566-a83f671d1d28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "for mnist_example in train_dataset.take(1):  # Only take a single example\n",
        "    image, label = mnist_example[\"image\"], mnist_example[\"label\"]\n",
        "\n",
        "    plt.imshow(image.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap(\"gray\"))\n",
        "    print(\"Label: %d\" % label.numpy())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMooC8y3rptU"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "NUM_EXAMPLES = 60000\n",
        "NUM_TESTING = 10000\n",
        "\n",
        "def preprocess(ds):\n",
        "    x = tf.cast(ds['image'], tf.float32) / 255.0\n",
        "    return x, ds['label']\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess).cache().repeat().shuffle(1024).batch(BATCH_SIZE)\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test_dataset.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM6jbMiADYuA"
      },
      "source": [
        "## Build simple fully connected layers (dense with dropout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h0qP2gM40xk"
      },
      "source": [
        "```\n",
        "Model: \"Your simple DNN\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "flatten_3 (Flatten)          (None, 784)               0         \n",
        "_________________________________________________________________\n",
        "dense_7 (Dense)              (None, 128)               100480    \n",
        "_________________________________________________________________\n",
        "dropout_5 (Dropout)          (None, 128)               0         \n",
        "_________________________________________________________________\n",
        "dense_8 (Dense)              (None, 64)                8256      \n",
        "_________________________________________________________________\n",
        "dense_9 (Dense)              (None, 10)                650       \n",
        "=================================================================\n",
        "Total params: 109,386\n",
        "Trainable params: 109,386\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFA6tiYB5-Ry"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, AveragePooling2D, Input, BatchNormalization\n",
        "\n",
        "# TODO\n",
        "def build_dnn():\n",
        "    model = tf.keras.models.Sequential([\n",
        "      # YOUR CODE HERE\n",
        "      Flatten(input_shape=(28,28,1)),\n",
        "      Dense(128,activation='relu'),\n",
        "      Dropout(0.2),\n",
        "      Dense(64,activation=\"relu\"),\n",
        "      Dense(10,activation=\"softmax\")\n",
        "\n",
        "      \n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E67nbvDpK6Fe",
        "outputId": "ac939831-5a60-4664-b471-94e460012ad2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = build_dnn()\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 109,386\n",
            "Trainable params: 109,386\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz2dVTSLr3wG",
        "outputId": "bd47ccc0-cc5f-464b-90ed-14941fb798b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train_dataset, epochs=20, steps_per_epoch=int(NUM_EXAMPLES/BATCH_SIZE),\n",
        "          validation_data=test_dataset, validation_steps=int(NUM_TESTING/BATCH_SIZE))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "468/468 [==============================] - 8s 16ms/step - loss: 0.3822 - accuracy: 0.8877 - val_loss: 0.1579 - val_accuracy: 0.9533\n",
            "Epoch 2/20\n",
            "468/468 [==============================] - 2s 5ms/step - loss: 0.1658 - accuracy: 0.9507 - val_loss: 0.1173 - val_accuracy: 0.9639\n",
            "Epoch 3/20\n",
            "468/468 [==============================] - 2s 5ms/step - loss: 0.1249 - accuracy: 0.9624 - val_loss: 0.0960 - val_accuracy: 0.9704\n",
            "Epoch 4/20\n",
            "468/468 [==============================] - 2s 5ms/step - loss: 0.0989 - accuracy: 0.9698 - val_loss: 0.0837 - val_accuracy: 0.9748\n",
            "Epoch 5/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.0840 - accuracy: 0.9740 - val_loss: 0.0756 - val_accuracy: 0.9763\n",
            "Epoch 6/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.0732 - accuracy: 0.9781 - val_loss: 0.0750 - val_accuracy: 0.9774\n",
            "Epoch 7/20\n",
            "468/468 [==============================] - 2s 3ms/step - loss: 0.0663 - accuracy: 0.9790 - val_loss: 0.0707 - val_accuracy: 0.9777\n",
            "Epoch 8/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.0574 - accuracy: 0.9818 - val_loss: 0.0682 - val_accuracy: 0.9800\n",
            "Epoch 9/20\n",
            "468/468 [==============================] - 2s 3ms/step - loss: 0.0533 - accuracy: 0.9826 - val_loss: 0.0668 - val_accuracy: 0.9794\n",
            "Epoch 10/20\n",
            "468/468 [==============================] - 2s 3ms/step - loss: 0.0487 - accuracy: 0.9839 - val_loss: 0.0716 - val_accuracy: 0.9793\n",
            "Epoch 11/20\n",
            "468/468 [==============================] - 2s 3ms/step - loss: 0.0447 - accuracy: 0.9849 - val_loss: 0.0631 - val_accuracy: 0.9818\n",
            "Epoch 12/20\n",
            "468/468 [==============================] - 2s 3ms/step - loss: 0.0407 - accuracy: 0.9862 - val_loss: 0.0678 - val_accuracy: 0.9814\n",
            "Epoch 13/20\n",
            "468/468 [==============================] - 2s 3ms/step - loss: 0.0399 - accuracy: 0.9866 - val_loss: 0.0644 - val_accuracy: 0.9819\n",
            "Epoch 14/20\n",
            "468/468 [==============================] - 2s 3ms/step - loss: 0.0359 - accuracy: 0.9879 - val_loss: 0.0730 - val_accuracy: 0.9789\n",
            "Epoch 15/20\n",
            "468/468 [==============================] - 2s 3ms/step - loss: 0.0355 - accuracy: 0.9883 - val_loss: 0.0703 - val_accuracy: 0.9808\n",
            "Epoch 16/20\n",
            "468/468 [==============================] - 2s 3ms/step - loss: 0.0318 - accuracy: 0.9888 - val_loss: 0.0691 - val_accuracy: 0.9818\n",
            "Epoch 17/20\n",
            "468/468 [==============================] - 2s 3ms/step - loss: 0.0309 - accuracy: 0.9892 - val_loss: 0.0725 - val_accuracy: 0.9804\n",
            "Epoch 18/20\n",
            "468/468 [==============================] - 2s 3ms/step - loss: 0.0303 - accuracy: 0.9895 - val_loss: 0.0709 - val_accuracy: 0.9818\n",
            "Epoch 19/20\n",
            "468/468 [==============================] - 2s 3ms/step - loss: 0.0291 - accuracy: 0.9902 - val_loss: 0.0752 - val_accuracy: 0.9804\n",
            "Epoch 20/20\n",
            "468/468 [==============================] - 2s 3ms/step - loss: 0.0286 - accuracy: 0.9900 - val_loss: 0.0790 - val_accuracy: 0.9807\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fad3a16acc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2qvl3ZbroX9"
      },
      "source": [
        "## Build LeNet-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr9e0KkB5s5n"
      },
      "source": [
        "![alt text](https://i.imgur.com/OD6XAUO.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSwdpiyk4_hc"
      },
      "source": [
        "```\n",
        "Model: \"LeNet\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "conv2d_38 (Conv2D)           (None, 24, 24, 20)        520       \n",
        "_________________________________________________________________\n",
        "max_pooling2d_23 (MaxPooling (None, 12, 12, 20)        0         \n",
        "_________________________________________________________________\n",
        "conv2d_39 (Conv2D)           (None, 8, 8, 50)          25050     \n",
        "_________________________________________________________________\n",
        "max_pooling2d_24 (MaxPooling (None, 4, 4, 50)          0         \n",
        "_________________________________________________________________\n",
        "flatten_6 (Flatten)          (None, 800)               0         \n",
        "_________________________________________________________________\n",
        "dense_15 (Dense)             (None, 500)               400500    \n",
        "_________________________________________________________________\n",
        "dense_16 (Dense)             (None, 10)                5010      \n",
        "=================================================================\n",
        "Total params: 431,080\n",
        "Trainable params: 431,080\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhR0fQ-0roI5"
      },
      "source": [
        "def build_lenet5():\n",
        "    model = tf.keras.models.Sequential()\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    model.add( Conv2D(kernel_size=(5,5),filters=20,activation='relu',input_shape=(28,28,1)) )\n",
        "    model.add( MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "    model.add(  Conv2D(kernel_size=(5,5),filters=50,activation='relu') )\n",
        "    model.add(  MaxPooling2D(pool_size=(2,2), strides=(2,2)) )\n",
        "\n",
        "    model.add( Flatten())\n",
        "    model.add( Dense(units=500,activation='relu'))\n",
        "    model.add( Dense(units=10,activation='softmax') )\n",
        "    return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEeH6T4JfQ-L",
        "outputId": "b58d4b4e-56bd-4933-e7f4-6cb056b5725e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lenet = build_lenet5()\n",
        "lenet.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 24, 24, 20)        520       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 12, 12, 20)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 8, 8, 50)          25050     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 50)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 500)               400500    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                5010      \n",
            "=================================================================\n",
            "Total params: 431,080\n",
            "Trainable params: 431,080\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmdO4WHMfWDY",
        "outputId": "b565ff58-88cd-4926-bb79-0902460540c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lenet.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "lenet.fit(train_dataset, epochs=20, steps_per_epoch=int(NUM_EXAMPLES/BATCH_SIZE),\n",
        "          validation_data=test_dataset, validation_steps=int(NUM_TESTING/BATCH_SIZE))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.1781 - accuracy: 0.9467 - val_loss: 0.0560 - val_accuracy: 0.9833\n",
            "Epoch 2/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.0462 - accuracy: 0.9855 - val_loss: 0.0383 - val_accuracy: 0.9881\n",
            "Epoch 3/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.0321 - accuracy: 0.9897 - val_loss: 0.0279 - val_accuracy: 0.9911\n",
            "Epoch 4/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.0237 - accuracy: 0.9925 - val_loss: 0.0336 - val_accuracy: 0.9896\n",
            "Epoch 5/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.0189 - accuracy: 0.9940 - val_loss: 0.0294 - val_accuracy: 0.9913\n",
            "Epoch 6/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.0145 - accuracy: 0.9955 - val_loss: 0.0287 - val_accuracy: 0.9922\n",
            "Epoch 7/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.0102 - accuracy: 0.9969 - val_loss: 0.0298 - val_accuracy: 0.9925\n",
            "Epoch 8/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.0096 - accuracy: 0.9970 - val_loss: 0.0326 - val_accuracy: 0.9920\n",
            "Epoch 9/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.0082 - accuracy: 0.9974 - val_loss: 0.0324 - val_accuracy: 0.9915\n",
            "Epoch 10/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.0075 - accuracy: 0.9975 - val_loss: 0.0377 - val_accuracy: 0.9906\n",
            "Epoch 11/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.0063 - accuracy: 0.9979 - val_loss: 0.0313 - val_accuracy: 0.9920\n",
            "Epoch 12/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.0073 - accuracy: 0.9974 - val_loss: 0.0332 - val_accuracy: 0.9915\n",
            "Epoch 13/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.0351 - val_accuracy: 0.9924\n",
            "Epoch 14/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 0.0443 - val_accuracy: 0.9902\n",
            "Epoch 15/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.0049 - accuracy: 0.9983 - val_loss: 0.0357 - val_accuracy: 0.9920\n",
            "Epoch 16/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.0051 - accuracy: 0.9984 - val_loss: 0.0375 - val_accuracy: 0.9913\n",
            "Epoch 17/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.0041 - accuracy: 0.9986 - val_loss: 0.0368 - val_accuracy: 0.9920\n",
            "Epoch 18/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.0040 - accuracy: 0.9985 - val_loss: 0.0397 - val_accuracy: 0.9928\n",
            "Epoch 19/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.0041 - accuracy: 0.9986 - val_loss: 0.0421 - val_accuracy: 0.9911\n",
            "Epoch 20/20\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0427 - val_accuracy: 0.9920\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fad39f02e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5chPg8ggb8m"
      },
      "source": [
        "lenet.save('lenet5.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJrf9o_gEB1p"
      },
      "source": [
        "##Build AlexNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtQG-oDZoUH0"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "Model: \"AlexNet\" \n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "conv2d_40 (Conv2D)           (None, 55, 55, 96)        34944     \n",
        "_________________________________________________________________\n",
        "max_pooling2d_18 (MaxPooling (None, 27, 27, 96)        0         \n",
        "_________________________________________________________________\n",
        "conv2d_41 (Conv2D)           (None, 27, 27, 256)       614656    \n",
        "_________________________________________________________________\n",
        "max_pooling2d_19 (MaxPooling (None, 13, 13, 256)       0         \n",
        "_________________________________________________________________\n",
        "conv2d_42 (Conv2D)           (None, 13, 13, 384)       885120    \n",
        "_________________________________________________________________\n",
        "conv2d_43 (Conv2D)           (None, 13, 13, 384)       1327488   \n",
        "_________________________________________________________________\n",
        "conv2d_44 (Conv2D)           (None, 13, 13, 256)       884992    \n",
        "_________________________________________________________________\n",
        "max_pooling2d_20 (MaxPooling (None, 6, 6, 256)         0         \n",
        "_________________________________________________________________\n",
        "flatten_7 (Flatten)          (None, 9216)              0         \n",
        "_________________________________________________________________\n",
        "dense_21 (Dense)             (None, 4096)              37752832  \n",
        "_________________________________________________________________\n",
        "dense_22 (Dense)             (None, 4096)              16781312  \n",
        "_________________________________________________________________\n",
        "dense_23 (Dense)             (None, 1000)              4097000   \n",
        "=================================================================\n",
        "Total params: 62,378,344\n",
        "Trainable params: 62,378,344\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nInc7isJ9gyL"
      },
      "source": [
        "![alt text](https://i.imgur.com/pHF2INm.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUcSJ14ToVus",
        "outputId": "d1a6ff3a-6056-41db-b885-e738e5dd7a3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def build_alexnet():\n",
        "    model = tf.keras.models.Sequential([\n",
        "      Conv2D(96,kernel_size=(11,11),strides=(4,4),input_shape=(227,227,3),activation='relu'),\n",
        "      MaxPooling2D(pool_size=(3,3),strides=(2,2)),\n",
        "\n",
        "      Conv2D(256,kernel_size=(5,5),padding='same',activation='relu'),\n",
        "      MaxPooling2D(pool_size=(3,3),strides=(2,2)),\n",
        "\n",
        "      Conv2D(384,kernel_size=(3,3),padding='same',activation='relu'),\n",
        "      Conv2D(384,kernel_size=(3,3),padding='same',activation='relu'),\n",
        "      Conv2D(256,kernel_size=(3,3),padding='same',activation='relu'),\n",
        "\n",
        "      MaxPooling2D(pool_size=(3,3),strides=(2,2)),\n",
        "\n",
        "      Flatten(),\n",
        "      Dense(4096, activation='relu'),\n",
        "      Dense(4096, activation='relu'),\n",
        "      Dense(1000, activation='softmax')\n",
        "      \n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_alexnet()\n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 55, 55, 96)        34944     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 27, 27, 96)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 27, 27, 256)       614656    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 13, 13, 384)       885120    \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 13, 13, 384)       1327488   \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 13, 13, 256)       884992    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 4096)              37752832  \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 62,378,344\n",
            "Trainable params: 62,378,344\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4P8e5QKoitL"
      },
      "source": [
        "### Challenge Time!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SExW1b5XpAkP"
      },
      "source": [
        "**CNN architecture**\n",
        "\n",
        "Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels.\n",
        "\n",
        "What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiWZcudnjhbx"
      },
      "source": [
        "# Your answer in number or words\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw30XfSSoXdT"
      },
      "source": [
        "## Build VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdo7aX_P5Qbh"
      },
      "source": [
        "![](https://media5.datahacker.rs/2019/02/vgg.png)\n",
        "\n",
        "```\n",
        "Model: \"vgg16\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "conv2d_159 (Conv2D)          (None, 224, 224, 64)      1792      \n",
        "_________________________________________________________________\n",
        "conv2d_160 (Conv2D)          (None, 224, 224, 64)      36928     \n",
        "_________________________________________________________________\n",
        "max_pooling2d_67 (MaxPooling (None, 112, 112, 64)      0         \n",
        "_________________________________________________________________\n",
        "conv2d_161 (Conv2D)          (None, 112, 112, 128)     73856     \n",
        "_________________________________________________________________\n",
        "conv2d_162 (Conv2D)          (None, 112, 112, 128)     147584    \n",
        "_________________________________________________________________\n",
        "max_pooling2d_68 (MaxPooling (None, 56, 56, 128)       0         \n",
        "_________________________________________________________________\n",
        "conv2d_163 (Conv2D)          (None, 56, 56, 256)       295168    \n",
        "_________________________________________________________________\n",
        "conv2d_164 (Conv2D)          (None, 56, 56, 256)       590080    \n",
        "_________________________________________________________________\n",
        "conv2d_165 (Conv2D)          (None, 56, 56, 256)       590080    \n",
        "_________________________________________________________________\n",
        "max_pooling2d_69 (MaxPooling (None, 28, 28, 256)       0         \n",
        "_________________________________________________________________\n",
        "conv2d_166 (Conv2D)          (None, 28, 28, 512)       1180160   \n",
        "_________________________________________________________________\n",
        "conv2d_167 (Conv2D)          (None, 28, 28, 512)       2359808   \n",
        "_________________________________________________________________\n",
        "conv2d_168 (Conv2D)          (None, 28, 28, 512)       2359808   \n",
        "_________________________________________________________________\n",
        "max_pooling2d_70 (MaxPooling (None, 14, 14, 512)       0         \n",
        "_________________________________________________________________\n",
        "conv2d_169 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
        "_________________________________________________________________\n",
        "conv2d_170 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
        "_________________________________________________________________\n",
        "conv2d_171 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
        "_________________________________________________________________\n",
        "max_pooling2d_71 (MaxPooling (None, 7, 7, 512)         0         \n",
        "_________________________________________________________________\n",
        "flatten_16 (Flatten)         (None, 25088)             0         \n",
        "_________________________________________________________________\n",
        "dense_39 (Dense)             (None, 4096)              102764544 \n",
        "_________________________________________________________________\n",
        "dense_40 (Dense)             (None, 4096)              16781312  \n",
        "_________________________________________________________________\n",
        "dense_41 (Dense)             (None, 1000)              4097000   \n",
        "=================================================================\n",
        "Total params: 138,357,544\n",
        "Trainable params: 138,357,544\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdBEUs0FjnbD"
      },
      "source": [
        "We load the real transfer learning VGG16 from Keras to get the parameters from it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2gG9bhNoZMF",
        "outputId": "e244fd10-50c1-4893-8199-b922ef3eaf1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "\n",
        "vgg16 = VGG16(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
        "vgg16.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 7s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ-PdRtRjuqH"
      },
      "source": [
        "Get the weights from the real VGG16 model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA8QI5wosFen"
      },
      "source": [
        "weights = vgg16.get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z2PQBKSsHyF"
      },
      "source": [
        "def build_vgg16():\n",
        "      model = tf.keras.models.Sequential([                \n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        \n",
        "  \n",
        "      ])\n",
        "      return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UvtLlBkj1SJ"
      },
      "source": [
        "If you build your model correctly then it can load the pretrained weigths of VGG16 without any error! It's gonna be impressive since you have build the VGG-16 from scratch!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrLnRrpqsMfx"
      },
      "source": [
        "model = build_vgg16()\n",
        "model.set_weights(weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "askbPnOI2EDx",
        "outputId": "5bc5d06d-11f9-4cc8-9b90-d52230a2e266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-yTB3dNDtkz"
      },
      "source": [
        "## Neural Style Transfer Art Competition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GoXbIPjowiE"
      },
      "source": [
        "**Go through TensorFlow's [Style Transfer tutorial](https://homl.info/styletuto)**"
      ]
    }
  ]
}