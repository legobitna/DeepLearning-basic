{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 10.2c_Tensorflow_Certificate_Question_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/legobitna/DeepLearning-basic/blob/main/10_2c_Tensorflow_Certificate_Question_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px-eBz-PX0xy"
      },
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score significantly\n",
        "# less than your Category 5 question.\n",
        "#\n",
        "# Don't use lambda layers in your model.\n",
        "# You do not need them to solve the question.\n",
        "# Lambda layers are not supported by the grading infrastructure.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ======================================================================\n",
        "#\n",
        "# NLP QUESTION\n",
        "#\n",
        "# Build and train a classifier for the sarcasm dataset.\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown.\n",
        "# It will be tested against a number of sentences that the network hasn't previously seen\n",
        "# and you will be scored on whether sarcasm was correctly detected in those sentences.\n",
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
        "\n",
        "    # with open('sarcasm.json) as f:\n",
        "    # content = f.readlines() \n",
        "\n",
        "    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type='post'\n",
        "    padding_type='post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_size = 20000\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    # YOUR CODE HERE\n",
        "    tokenizer = Tokenizer(num_words=vocab_size)\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    word_index = tokenizer.word_index\n",
        "    sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "    padded = pad_sequences(sequences, padding=padding_type, truncating=trunc_type, maxlen=max_length)\n",
        "\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "    # YOUR CODE HERE. KEEP THIS OUTPUT LAYER INTACT OR TESTS MAY FAIL\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        # your code\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "        tf.keras.layers.Dense(8,activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "  \n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZFhp6KeGj3K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Pj_D4ursCKn"
      },
      "source": [
        "import urllib.request\n",
        "import json\n",
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "urllib.request.urlretrieve(url, 'sarcasm.json')\n",
        "\n",
        "with open('sarcasm.json') as f:\n",
        "  content = json.load(f)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brqwPIIxsLkM"
      },
      "source": [
        "labels = list(map(lambda x: x['is_sarcastic'], content))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADwWOVSBt-xe"
      },
      "source": [
        "data = list(map(lambda x: x['headline'], content))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP-cAwISvEkJ",
        "outputId": "e8fe829c-917e-43e4-b77e-ebb1a19d5b8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data[:5]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"former versace store clerk sues over secret 'black code' for minority shoppers\",\n",
              " \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\",\n",
              " \"mom starting to fear son's web series closest thing she will have to grandchild\",\n",
              " 'boehner just wants wife to listen, not come up with alternative debt-reduction ideas',\n",
              " 'j.k. rowling wishes snape happy birthday in the most magical way']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuyNEJmPvFYj"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(data)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(data)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYFza0rMv9TC",
        "outputId": "0b882538-505a-4aad-a72c-04e38abf16d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.array(sequences).shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26709,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCPI50e9v_GW"
      },
      "source": [
        "# DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
        "vocab_size = 1000\n",
        "embedding_dim = 16\n",
        "max_length = 120\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "training_size = 20000\n",
        "\n",
        "# sentences = []\n",
        "# labels = []\n",
        "# YOUR CODE HERE\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(data)\n",
        "word_index = tokenizer.word_index\n",
        "data = tokenizer.texts_to_sequences(data)\n",
        "\n",
        "padded = pad_sequences(data, padding=padding_type, truncating=trunc_type, maxlen=max_length)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eSitc1ZwHaF",
        "outputId": "cafa187b-fb93-4bea-dd92-5c22bd35f004",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "padded.shape[0]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26709"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqh2lrkzwY-N"
      },
      "source": [
        "# dataset = tf.data.Dataset.from_tensor_slices((padded,labels))\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wML9aYK9wncf",
        "outputId": "595be1a0-eabe-4a5c-cc7a-9c56ab975cc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: ((120,), ()), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VtRdvM7ws0I",
        "outputId": "fc8945a2-1384-4e22-a6e2-76440d6d7b51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_size = int(0.8 * padded.shape[0])\n",
        "val_size = int(0.1 *  padded.shape[0])\n",
        "test_size = int(0.1 *  padded.shape[0])\n",
        "\n",
        "\n",
        "# train_dataset = padded.take(train_size)\n",
        "# test_dataset = padded.skip(train_size)\n",
        "# val_dataset = test_dataset.skip(val_size)\n",
        "# test_dataset = test_dataset.take(test_size)\n",
        "\n",
        "X_train, X_test, y_train, y_test= train_test_split(padded, labels, test_size=0.1, random_state=42)\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "X_train=np.array(X_train)\n",
        "X_val=np.array(X_val)\n",
        "y_train=np.array(y_train)\n",
        "y_val=np.array(y_val)\n",
        "\n",
        "print(X_train.shape)\n",
        "model=solution_model()\n",
        "model.fit(X_train,y_train,epochs=10, validation_data=(X_val, y_val))\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21634, 120)\n",
            "Epoch 1/10\n",
            "677/677 [==============================] - 129s 190ms/step - loss: 0.4606 - accuracy: 0.7749 - val_loss: 0.4058 - val_accuracy: 0.8111\n",
            "Epoch 2/10\n",
            "677/677 [==============================] - 128s 189ms/step - loss: 0.3733 - accuracy: 0.8273 - val_loss: 0.3607 - val_accuracy: 0.8336\n",
            "Epoch 3/10\n",
            "677/677 [==============================] - 128s 190ms/step - loss: 0.3592 - accuracy: 0.8341 - val_loss: 0.3569 - val_accuracy: 0.8324\n",
            "Epoch 4/10\n",
            "677/677 [==============================] - 134s 198ms/step - loss: 0.3382 - accuracy: 0.8473 - val_loss: 0.3442 - val_accuracy: 0.8390\n",
            "Epoch 5/10\n",
            "677/677 [==============================] - 129s 190ms/step - loss: 0.3263 - accuracy: 0.8531 - val_loss: 0.3460 - val_accuracy: 0.8419\n",
            "Epoch 6/10\n",
            "677/677 [==============================] - 129s 190ms/step - loss: 0.3170 - accuracy: 0.8558 - val_loss: 0.3458 - val_accuracy: 0.8440\n",
            "Epoch 7/10\n",
            "677/677 [==============================] - 129s 190ms/step - loss: 0.3083 - accuracy: 0.8606 - val_loss: 0.3526 - val_accuracy: 0.8394\n",
            "Epoch 8/10\n",
            "677/677 [==============================] - 129s 190ms/step - loss: 0.3005 - accuracy: 0.8636 - val_loss: 0.3636 - val_accuracy: 0.8369\n",
            "Epoch 9/10\n",
            "677/677 [==============================] - 135s 199ms/step - loss: 0.2931 - accuracy: 0.8664 - val_loss: 0.3704 - val_accuracy: 0.8344\n",
            "Epoch 10/10\n",
            "677/677 [==============================] - 129s 191ms/step - loss: 0.2868 - accuracy: 0.8683 - val_loss: 0.3806 - val_accuracy: 0.8282\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc37ea1ad30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1Gl2GOXJSTA"
      },
      "source": [
        "model.save(\"bitnamoel.h5\")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybdSyUDjJYPw"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOGoOFX4EQ59",
        "outputId": "e2631c40-0901-4cf5-9c61-031b11bf8bb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_test=np.array(X_test)\n",
        "y_test=np.array(y_test)\n",
        "\n",
        "\n",
        "model.predict(X_test[0])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 120) for input Tensor(\"embedding_12_input:0\", shape=(None, 120), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.38389668],\n",
              "       [0.83264554],\n",
              "       [0.55794054],\n",
              "       [0.3543154 ],\n",
              "       [0.3031619 ],\n",
              "       [0.55794054],\n",
              "       [0.5270376 ],\n",
              "       [0.46883637],\n",
              "       [0.6002087 ],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474],\n",
              "       [0.47972474]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    }
  ]
}